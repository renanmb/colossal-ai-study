{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrFehJ8lEHqe"
   },
   "source": [
    "# Hybrid Parallelism\n",
    "\n",
    "Example of hybriding pipeline and 1-D (2-D) tensor parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLTXVsVjENKE",
    "outputId": "58ea5654-2679-4221-b894-19703cb2adb1"
   },
   "outputs": [],
   "source": [
    "%cd /workshop/tutorial/hybrid_parallel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open [config.py](hybrid_parallel/config.py) or click on `...` to view it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "from colossalai.amp import AMP_TYPE\n",
    "\n",
    "# hyperparameters\n",
    "# BATCH_SIZE is as per GPU\n",
    "# global batch size = BATCH_SIZE x data parallel size\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 3e-3\n",
    "WEIGHT_DECAY = 0.3\n",
    "NUM_EPOCHS = 2\n",
    "WARMUP_EPOCHS = 1\n",
    "\n",
    "# model config\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "HIDDEN_SIZE = 128\n",
    "DEPTH = 4\n",
    "NUM_HEADS = 4\n",
    "MLP_RATIO = 2\n",
    "NUM_CLASSES = 10\n",
    "CHECKPOINT = False\n",
    "SEQ_LENGTH = (IMG_SIZE // PATCH_SIZE)**2 + 1    # add 1 for cls token\n",
    "\n",
    "# parallel setting\n",
    "TENSOR_PARALLEL_SIZE = 2\n",
    "TENSOR_PARALLEL_MODE = '1d'\n",
    "\n",
    "parallel = dict(\n",
    "    pipeline=1,\n",
    "    tensor=dict(mode=TENSOR_PARALLEL_MODE, size=TENSOR_PARALLEL_SIZE),\n",
    ")\n",
    "\n",
    "fp16 = dict(mode=AMP_TYPE.NAIVE)\n",
    "clip_grad_norm = 1.0\n",
    "\n",
    "# pipeline config\n",
    "NUM_MICRO_BATCHES = parallel['pipeline']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open [train.py](hybrid_parallel/train.py) or click on `...` to view it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from titans.model.vit.vit import _create_vit_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "import colossalai\n",
    "from colossalai.context import ParallelMode\n",
    "from colossalai.core import global_context as gpc\n",
    "from colossalai.logging import get_dist_logger\n",
    "from colossalai.nn import CrossEntropyLoss\n",
    "from colossalai.nn.lr_scheduler import CosineAnnealingWarmupLR\n",
    "from colossalai.pipeline.pipelinable import PipelinableContext\n",
    "from colossalai.utils import is_using_pp\n",
    "\n",
    "\n",
    "class DummyDataloader():\n",
    "\n",
    "    def __init__(self, length, batch_size):\n",
    "        self.length = length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate(self):\n",
    "        data = torch.rand(self.batch_size, 3, 224, 224)\n",
    "        label = torch.randint(low=0, high=10, size=(self.batch_size,))\n",
    "        return data, label\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.step = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.step < self.length:\n",
    "            self.step += 1\n",
    "            return self.generate()\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "def main():\n",
    "    # launch from torch\n",
    "    parser = colossalai.get_default_parser()\n",
    "    args = parser.parse_args()\n",
    "    colossalai.launch_from_torch(config=args.config)\n",
    "\n",
    "    # get logger\n",
    "    logger = get_dist_logger()\n",
    "    logger.info(\"initialized distributed environment\", ranks=[0])\n",
    "\n",
    "    if hasattr(gpc.config, 'LOG_PATH'):\n",
    "        if gpc.get_global_rank() == 0:\n",
    "            log_path = gpc.config.LOG_PATH\n",
    "            if not os.path.exists(log_path):\n",
    "                os.mkdir(log_path)\n",
    "            logger.log_to_file(log_path)\n",
    "\n",
    "    use_pipeline = is_using_pp()\n",
    "\n",
    "    # create model\n",
    "    model_kwargs = dict(img_size=gpc.config.IMG_SIZE,\n",
    "                        patch_size=gpc.config.PATCH_SIZE,\n",
    "                        hidden_size=gpc.config.HIDDEN_SIZE,\n",
    "                        depth=gpc.config.DEPTH,\n",
    "                        num_heads=gpc.config.NUM_HEADS,\n",
    "                        mlp_ratio=gpc.config.MLP_RATIO,\n",
    "                        num_classes=10,\n",
    "                        init_method='jax',\n",
    "                        checkpoint=gpc.config.CHECKPOINT)\n",
    "\n",
    "    if use_pipeline:\n",
    "        pipelinable = PipelinableContext()\n",
    "        with pipelinable:\n",
    "            model = _create_vit_model(**model_kwargs)\n",
    "        pipelinable.to_layer_list()\n",
    "        pipelinable.policy = \"uniform\"\n",
    "        model = pipelinable.partition(1, gpc.pipeline_parallel_size, gpc.get_local_rank(ParallelMode.PIPELINE))\n",
    "    else:\n",
    "        model = _create_vit_model(**model_kwargs)\n",
    "\n",
    "    # count number of parameters\n",
    "    total_numel = 0\n",
    "    for p in model.parameters():\n",
    "        total_numel += p.numel()\n",
    "    if not gpc.is_initialized(ParallelMode.PIPELINE):\n",
    "        pipeline_stage = 0\n",
    "    else:\n",
    "        pipeline_stage = gpc.get_local_rank(ParallelMode.PIPELINE)\n",
    "    logger.info(f\"number of parameters: {total_numel} on pipeline stage {pipeline_stage}\")\n",
    "\n",
    "    # use synthetic dataset\n",
    "    # we train for 10 steps and eval for 5 steps per epoch\n",
    "    train_dataloader = DummyDataloader(length=10, batch_size=gpc.config.BATCH_SIZE)\n",
    "    test_dataloader = DummyDataloader(length=5, batch_size=gpc.config.BATCH_SIZE)\n",
    "\n",
    "    # create loss function\n",
    "    criterion = CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    # create optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=gpc.config.LEARNING_RATE, weight_decay=gpc.config.WEIGHT_DECAY)\n",
    "\n",
    "    # create lr scheduler\n",
    "    lr_scheduler = CosineAnnealingWarmupLR(optimizer=optimizer,\n",
    "                                           total_steps=gpc.config.NUM_EPOCHS,\n",
    "                                           warmup_steps=gpc.config.WARMUP_EPOCHS)\n",
    "\n",
    "    # initialize\n",
    "    engine, train_dataloader, test_dataloader, _ = colossalai.initialize(model=model,\n",
    "                                                                         optimizer=optimizer,\n",
    "                                                                         criterion=criterion,\n",
    "                                                                         train_dataloader=train_dataloader,\n",
    "                                                                         test_dataloader=test_dataloader)\n",
    "\n",
    "    logger.info(\"Engine is built\", ranks=[0])\n",
    "\n",
    "    for epoch in range(gpc.config.NUM_EPOCHS):\n",
    "        # training\n",
    "        engine.train()\n",
    "        data_iter = iter(train_dataloader)\n",
    "\n",
    "        if gpc.get_global_rank() == 0:\n",
    "            description = 'Epoch {} / {}'.format(epoch, gpc.config.NUM_EPOCHS)\n",
    "            progress = tqdm(range(len(train_dataloader)), desc=description)\n",
    "        else:\n",
    "            progress = range(len(train_dataloader))\n",
    "        for _ in progress:\n",
    "            engine.zero_grad()\n",
    "            engine.execute_schedule(data_iter, return_output_label=False)\n",
    "            engine.step()\n",
    "            lr_scheduler.step()\n",
    "    gpc.destroy()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOs1FbvvEsGP",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute example trial\n",
    "!colossalai run --nproc_per_node 2 train.py --config config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRgTRMX0FiP0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's now tweak the configuation to adopt 2-D tensor parallelism\n",
    "# the following example require 4 GPUs, please uncomment the lines below only if you have 4 GPUs available\n",
    "\n",
    "# with open('config.py', 'r') as file:\n",
    "#   data = file.readlines()\n",
    "\n",
    "# # Change line 24-25\n",
    "# data[23] = 'TENSOR_PARALLEL_SIZE = 4\\n'\n",
    "# data[24] = \"TENSOR_PARALLEL_MODE = '2d'\\n\"\n",
    "\n",
    "# with open('config.py', 'w') as file:\n",
    "#   file.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7s-rxf_SHBrj",
    "outputId": "ff721f05-c1a9-403a-b579-240717dcc889",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# New trial with hybrid of 2-D tensor parallism with pipeline parallelism \n",
    "\n",
    "# !colossalai run --nproc_per_node 8 train.py --config config.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "933351abc630374ba7d69d29305d8827306c5eb75384e02fe79e055db4d99722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
