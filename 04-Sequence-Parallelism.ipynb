{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1qbh54oHbNf"
   },
   "source": [
    "# Sequence Parallelism\n",
    "\n",
    "Interested users may refer to [this paper](https://arxiv.org/abs/2105.13120) for implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRmQTlZnHa3V",
    "outputId": "35cca02d-9f24-4b5d-efc7-2c057b153ae7"
   },
   "outputs": [],
   "source": [
    "%cd /workshop/tutorial/sequence_parallel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-ZwjFrfIVOG",
    "outputId": "0f837a3d-efa8-4971-97c9-09aa02d58be1"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open [config.py](sequence_parallel/config.py) or click on `...` to view it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "from colossalai.amp import AMP_TYPE\n",
    "\n",
    "# hyperparameters\n",
    "# BATCH_SIZE is as per GPU\n",
    "# global batch size = BATCH_SIZE x data parallel size\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 3e-3\n",
    "WEIGHT_DECAY = 0.3\n",
    "NUM_EPOCHS = 2\n",
    "WARMUP_EPOCHS = 1\n",
    "\n",
    "# model config\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "HIDDEN_SIZE = 128\n",
    "DEPTH = 4\n",
    "NUM_HEADS = 4\n",
    "MLP_RATIO = 2\n",
    "NUM_CLASSES = 10\n",
    "CHECKPOINT = False\n",
    "SEQ_LENGTH = (IMG_SIZE // PATCH_SIZE)**2 + 1    # add 1 for cls token\n",
    "\n",
    "# parallel setting\n",
    "TENSOR_PARALLEL_SIZE = 2\n",
    "TENSOR_PARALLEL_MODE = '1d'\n",
    "\n",
    "parallel = dict(\n",
    "    pipeline=1,\n",
    "    tensor=dict(mode=TENSOR_PARALLEL_MODE, size=TENSOR_PARALLEL_SIZE),\n",
    ")\n",
    "\n",
    "fp16 = dict(mode=AMP_TYPE.NAIVE)\n",
    "clip_grad_norm = 1.0\n",
    "\n",
    "# pipeline config\n",
    "NUM_MICRO_BATCHES = parallel['pipeline']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open [train.py](sequence_parallel/train.py) or click on `...` to view it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from data.bert_helper import SequenceParallelDataIterator, get_batch_for_sequence_parallel\n",
    "from data.dummy_dataloader import DummyDataloader\n",
    "from loss_func.bert_loss import BertLoss\n",
    "from lr_scheduler import AnnealingLR\n",
    "from model.bert import BertForPretrain, build_pipeline_bert\n",
    "\n",
    "import colossalai\n",
    "from colossalai.amp import AMP_TYPE\n",
    "from colossalai.context.parallel_mode import ParallelMode\n",
    "from colossalai.core import global_context as gpc\n",
    "from colossalai.engine.schedule import PipelineSchedule\n",
    "from colossalai.kernel import LayerNorm\n",
    "from colossalai.logging import get_dist_logger\n",
    "from colossalai.nn.optimizer import FusedAdam\n",
    "from colossalai.utils import MultiTimer, is_using_pp\n",
    "\n",
    "\n",
    "def process_batch_data(batch_data):\n",
    "    tokens, types, sentence_order, loss_mask, lm_labels, padding_mask = batch_data\n",
    "    if gpc.is_first_rank(ParallelMode.PIPELINE):\n",
    "        data = dict(input_ids=tokens, attention_masks=padding_mask, tokentype_ids=types, lm_labels=lm_labels)\n",
    "    else:\n",
    "        data = dict(attention_masks=padding_mask, tokentype_ids=types, lm_labels=lm_labels)\n",
    "    label = dict(loss_mask=loss_mask, sentence_order=sentence_order)\n",
    "    return data, label\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-s', '--synthetic', action=\"store_true\", help=\"whether use synthetic data\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def pipeline_data_process_func(stage_output, micro_batch_data):\n",
    "    tokens, types, sentence_order, loss_mask, lm_labels, padding_mask = micro_batch_data\n",
    "    if gpc.is_first_rank(ParallelMode.PIPELINE):\n",
    "        data = (tokens, padding_mask, types, lm_labels)\n",
    "        label = (loss_mask, sentence_order)\n",
    "    else:\n",
    "        data = (stage_output, padding_mask, types, lm_labels)\n",
    "        label = (loss_mask, sentence_order)\n",
    "    return data, label\n",
    "\n",
    "\n",
    "def main():\n",
    "    # initialize\n",
    "    args = parse_args()\n",
    "    colossalai.launch_from_torch(config='./config.py', seed=1234, backend='nccl')\n",
    "\n",
    "    logger = get_dist_logger()\n",
    "\n",
    "    # build synthetic dataloader\n",
    "    BATCH_SIZE_PER_GPUS = gpc.config.GLOBAL_BATCH_SIZE // gpc.get_world_size(ParallelMode.DATA)\n",
    "    VOCAB_SIZE = 30528\n",
    "    trainloader = DummyDataloader(batch_size=BATCH_SIZE_PER_GPUS,\n",
    "                                  vocab_size=VOCAB_SIZE,\n",
    "                                  seq_length=gpc.config.SEQ_LENGTH)\n",
    "    validloader = DummyDataloader(batch_size=BATCH_SIZE_PER_GPUS,\n",
    "                                  vocab_size=VOCAB_SIZE,\n",
    "                                  seq_length=gpc.config.SEQ_LENGTH)\n",
    "\n",
    "    logger.info(\"Dataloaders are built\", ranks=[0])\n",
    "\n",
    "    # build model\n",
    "    if hasattr(gpc.config, 'fp16') and gpc.config.fp16.get('mode') == AMP_TYPE.NAIVE:\n",
    "        is_naive_fp16 = True\n",
    "    else:\n",
    "        is_naive_fp16 = False\n",
    "\n",
    "    use_pipeline = is_using_pp()\n",
    "    kwargs = dict(vocab_size=VOCAB_SIZE,\n",
    "                  hidden_size=gpc.config.HIDDEN_SIZE,\n",
    "                  max_sequence_length=gpc.config.SEQ_LENGTH,\n",
    "                  num_attention_heads=gpc.config.NUM_ATTENTION_HEADS,\n",
    "                  convert_fp16_to_fp32_in_softmax=True,\n",
    "                  is_naive_fp16=is_naive_fp16,\n",
    "                  add_binary_head=gpc.config.ADD_BINARY_HEAD)\n",
    "\n",
    "    if use_pipeline:\n",
    "        model = build_pipeline_bert(num_layers=gpc.config.DEPTH, num_chunks=1, **kwargs)\n",
    "    else:\n",
    "        model = BertForPretrain(num_layers=gpc.config.DEPTH, **kwargs)\n",
    "\n",
    "    model = model.half()\n",
    "    model.reset_parameters()\n",
    "    logger.info(f\"Model is built with softmax in fp32 = {is_naive_fp16}\", ranks=[0])\n",
    "\n",
    "    total_numel = 0\n",
    "    for p in model.parameters():\n",
    "        total_numel += p.numel()\n",
    "    logger.info(f\"This model has {total_numel} parameters\")\n",
    "\n",
    "    # build criterion\n",
    "    criterion = BertLoss()\n",
    "    logger.info(\"Criterion is built\", ranks=[0])\n",
    "\n",
    "    # layernorm and bias has no weight decay\n",
    "    weight_decay_params = {'params': []}\n",
    "    no_weight_decay_params = {'params': [], 'weight_decay': 0.0}\n",
    "    for module_ in model.modules():\n",
    "        if isinstance(module_, LayerNorm):\n",
    "            no_weight_decay_params['params'].extend([p for p in list(module_._parameters.values()) if p is not None])\n",
    "        else:\n",
    "            weight_decay_params['params'].extend(\n",
    "                [p for n, p in list(module_._parameters.items()) if p is not None and n != 'bias'])\n",
    "            no_weight_decay_params['params'].extend(\n",
    "                [p for n, p in list(module_._parameters.items()) if p is not None and n == 'bias'])\n",
    "\n",
    "    logger.info(\n",
    "        f\"without weight decay param: {len(no_weight_decay_params['params'])}, with weight decay param: {len(weight_decay_params['params'])}\"\n",
    "    )\n",
    "    # optimizer\n",
    "    optimizer = FusedAdam((weight_decay_params, no_weight_decay_params),\n",
    "                          lr=gpc.config.LR,\n",
    "                          weight_decay=gpc.config.WEIGHT_DECAY)\n",
    "    logger.info(\"Optimizer is built\", ranks=[0])\n",
    "\n",
    "    # lr scheduler\n",
    "    # follow Megatron-LM setting\n",
    "    warmup_steps = int(gpc.config.DECAY_ITERS * gpc.config.WARMUP_FRACTION)\n",
    "    lr_scheduler = AnnealingLR(optimizer=optimizer,\n",
    "                               max_lr=gpc.config.LR,\n",
    "                               min_lr=gpc.config.MIN_LR,\n",
    "                               warmup_steps=warmup_steps,\n",
    "                               decay_steps=gpc.config.DECAY_ITERS,\n",
    "                               decay_style='linear')\n",
    "    logger.info(f\"LR Scheduler is built with {warmup_steps} warmup steps and {gpc.config.DECAY_ITERS} decay steps\")\n",
    "\n",
    "    # # init\n",
    "    engine, *dummy = colossalai.initialize(model, optimizer, criterion, verbose=True)\n",
    "\n",
    "    # build timer\n",
    "    timer = MultiTimer()\n",
    "    skip_iters = 0\n",
    "\n",
    "    # build loss tracker\n",
    "    accumulated_train_loss = torch.zeros(1, dtype=torch.float32).cuda()\n",
    "    accumulated_eval_loss = torch.zeros(1, dtype=torch.float32).cuda()\n",
    "\n",
    "    # build data iters for pipeline parallel\n",
    "    if use_pipeline:\n",
    "        train_data_iter = SequenceParallelDataIterator(trainloader)\n",
    "        valid_data_iter = SequenceParallelDataIterator(validloader)\n",
    "        engine.schedule.data_process_func = pipeline_data_process_func\n",
    "\n",
    "    logger.info(\"start training\")\n",
    "\n",
    "    for step in range(1, gpc.config.TRAIN_ITERS + 1):\n",
    "        timer.start('train-iterations')\n",
    "        engine.train()\n",
    "        if use_pipeline:\n",
    "            engine.zero_grad()\n",
    "            _, _, train_loss = engine.execute_schedule(train_data_iter, return_output_label=False)\n",
    "            engine.step()\n",
    "        else:\n",
    "            tokens, types, sentence_order, loss_mask, lm_labels, padding_mask = get_batch_for_sequence_parallel(\n",
    "                trainloader)\n",
    "            engine.zero_grad()\n",
    "            lm_loss, sop_output = engine(tokens, padding_mask, types, lm_labels)\n",
    "            train_loss = engine.criterion(lm_loss, sop_output, loss_mask, sentence_order)\n",
    "            engine.backward(train_loss)\n",
    "            engine.step()\n",
    "        timer.stop('train-iterations', keep_in_history=True)\n",
    "\n",
    "        if not gpc.is_initialized(ParallelMode.PIPELINE) or gpc.is_last_rank(ParallelMode.PIPELINE):\n",
    "            accumulated_train_loss += train_loss\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if step % gpc.config.EVAL_INTERVAL == 0:\n",
    "            engine.eval()\n",
    "\n",
    "            for j in range(gpc.config.EVAL_ITERS):\n",
    "                with torch.no_grad():\n",
    "                    if use_pipeline:\n",
    "                        _, _, eval_loss = engine.execute_schedule(valid_data_iter,\n",
    "                                                                  forward_only=True,\n",
    "                                                                  return_output_label=False)\n",
    "                    else:\n",
    "                        tokens, types, sentence_order, loss_mask, lm_labels, padding_mask = get_batch_for_sequence_parallel(\n",
    "                            validloader)\n",
    "                        lm_loss, sop_output = engine(tokens, padding_mask, types, lm_labels)\n",
    "                        eval_loss = engine.criterion(lm_loss, sop_output, loss_mask, sentence_order)\n",
    "\n",
    "                    if not gpc.is_initialized(ParallelMode.PIPELINE) or gpc.is_last_rank(ParallelMode.PIPELINE):\n",
    "                        accumulated_eval_loss += eval_loss\n",
    "\n",
    "            if not gpc.is_initialized(ParallelMode.PIPELINE) or gpc.is_last_rank(ParallelMode.PIPELINE):\n",
    "                accumulated_eval_loss /= gpc.config.EVAL_ITERS\n",
    "                accumulated_train_loss /= gpc.config.EVAL_INTERVAL\n",
    "\n",
    "            timer_string = []\n",
    "            for n, t in timer:\n",
    "                timer_string.append(f\"{n}: {t.get_history_mean()*1000:.5f}\")\n",
    "            timer_string = ' | '.join(timer_string)\n",
    "            lr = list(engine.optimizer.param_groups)[0]['lr']\n",
    "            loss_scale = engine.optimizer.optim.loss_scale.item()\n",
    "\n",
    "            if gpc.is_initialized(ParallelMode.PIPELINE):\n",
    "                ranks = [gpc.get_ranks_in_group(ParallelMode.PIPELINE)[-1]]\n",
    "            else:\n",
    "                ranks = [0]\n",
    "            logger.info(f'Step {step} / {gpc.config.TRAIN_ITERS} | Train Loss: {accumulated_train_loss.item():.5g} ' +\n",
    "                        f'| Eval Loss: {accumulated_eval_loss.item():.5g} ' + f'| Loss Scale: {loss_scale}' +\n",
    "                        f\"| Learning rate: {lr} | \" + timer_string,\n",
    "                        ranks=ranks)\n",
    "\n",
    "            for n, t in timer:\n",
    "                t.reset()\n",
    "            accumulated_eval_loss.zero_()\n",
    "            accumulated_train_loss.zero_()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Quhl6RAhH9p3",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vanilla trial: sequence parallelism (2) \n",
    "# 10 steps only\n",
    "!colossalai run --nproc_per_node 2 train.py -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqRe_ZcXIxFB"
   },
   "outputs": [],
   "source": [
    "# Let's now tweak the configuation to adopt size=2 pipeline parallelism\n",
    "# The following code is commented out because 4 GPUs are needed\n",
    "# uncomment them only if you have 4 GPUs\n",
    "\n",
    "# with open('config.py', 'r') as file:\n",
    "#   data = file.readlines()\n",
    "\n",
    "# # Change line 31\n",
    "# data[30] = \"parallel = dict(pipeline=2, tensor=dict(size=2, mode='sequence'))\\n\"\n",
    "\n",
    "# with open('config.py', 'w') as file:\n",
    "#   file.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIealehOJV9f"
   },
   "outputs": [],
   "source": [
    "# Run trial again: sequence parallelism (2) x pipeline parallelism (2)\n",
    "\n",
    "# !colossalai run --nproc_per_node 4 train.py -s"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "933351abc630374ba7d69d29305d8827306c5eb75384e02fe79e055db4d99722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
