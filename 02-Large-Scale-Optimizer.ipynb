{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-9b4ciAAPaE"
   },
   "source": [
    "# Large-Scale Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-VM4iD3AV9u",
    "outputId": "1ef27f11-f7bc-4f02-871c-3ccba41ae395"
   },
   "outputs": [],
   "source": [
    "%cd /workshop/tutorial/large_batch_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39RptxqLBOQ1",
    "outputId": "4e62ec52-d86b-4cea-fd1b-69b032640575",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of files in the ColossalAI/examples/tutorial/large_batch_optimizer directory\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open [config.py](large_batch_optimizer/config.py) or click on `...` to view it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "from colossalai.amp import AMP_TYPE\n",
    "\n",
    "# hyperparameters\n",
    "# BATCH_SIZE is as per GPU\n",
    "# global batch size = BATCH_SIZE x data parallel size\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 3e-3\n",
    "WEIGHT_DECAY = 0.3\n",
    "NUM_EPOCHS = 2\n",
    "WARMUP_EPOCHS = 1\n",
    "\n",
    "# model config\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "fp16 = dict(mode=AMP_TYPE.NAIVE)\n",
    "clip_grad_norm = 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open [train.py](large_batch_optimizer/train.py) or click on `...` to view it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from tqdm import tqdm\n",
    "\n",
    "import colossalai\n",
    "from colossalai.core import global_context as gpc\n",
    "from colossalai.logging import get_dist_logger\n",
    "from colossalai.nn.lr_scheduler import CosineAnnealingWarmupLR\n",
    "from colossalai.nn.optimizer import Lamb, Lars\n",
    "\n",
    "\n",
    "class DummyDataloader():\n",
    "\n",
    "    def __init__(self, length, batch_size):\n",
    "        self.length = length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate(self):\n",
    "        data = torch.rand(self.batch_size, 3, 224, 224)\n",
    "        label = torch.randint(low=0, high=10, size=(self.batch_size,))\n",
    "        return data, label\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.step = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.step < self.length:\n",
    "            self.step += 1\n",
    "            return self.generate()\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "def main():\n",
    "    # initialize distributed setting\n",
    "    parser = colossalai.get_default_parser()\n",
    "    parser.add_argument('--optimizer',\n",
    "                        choices=['lars', 'lamb'],\n",
    "                        help=\"Choose your large-batch optimizer\",\n",
    "                        required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # launch from torch\n",
    "    colossalai.launch_from_torch(config=args.config)\n",
    "\n",
    "    # get logger\n",
    "    logger = get_dist_logger()\n",
    "    logger.info(\"initialized distributed environment\", ranks=[0])\n",
    "\n",
    "    # create synthetic dataloaders\n",
    "    train_dataloader = DummyDataloader(length=10, batch_size=gpc.config.BATCH_SIZE)\n",
    "    test_dataloader = DummyDataloader(length=5, batch_size=gpc.config.BATCH_SIZE)\n",
    "\n",
    "    # build model\n",
    "    model = resnet18(num_classes=gpc.config.NUM_CLASSES)\n",
    "\n",
    "    # create loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # create optimizer\n",
    "    if args.optimizer == \"lars\":\n",
    "        optim_cls = Lars\n",
    "    elif args.optimizer == \"lamb\":\n",
    "        optim_cls = Lamb\n",
    "    optimizer = optim_cls(model.parameters(), lr=gpc.config.LEARNING_RATE, weight_decay=gpc.config.WEIGHT_DECAY)\n",
    "\n",
    "    # create lr scheduler\n",
    "    lr_scheduler = CosineAnnealingWarmupLR(optimizer=optimizer,\n",
    "                                           total_steps=gpc.config.NUM_EPOCHS,\n",
    "                                           warmup_steps=gpc.config.WARMUP_EPOCHS)\n",
    "\n",
    "    # initialize\n",
    "    engine, train_dataloader, test_dataloader, _ = colossalai.initialize(model=model,\n",
    "                                                                         optimizer=optimizer,\n",
    "                                                                         criterion=criterion,\n",
    "                                                                         train_dataloader=train_dataloader,\n",
    "                                                                         test_dataloader=test_dataloader)\n",
    "\n",
    "    logger.info(\"Engine is built\", ranks=[0])\n",
    "\n",
    "    for epoch in range(gpc.config.NUM_EPOCHS):\n",
    "        # training\n",
    "        engine.train()\n",
    "        data_iter = iter(train_dataloader)\n",
    "\n",
    "        if gpc.get_global_rank() == 0:\n",
    "            description = 'Epoch {} / {}'.format(epoch, gpc.config.NUM_EPOCHS)\n",
    "            progress = tqdm(range(len(train_dataloader)), desc=description)\n",
    "        else:\n",
    "            progress = range(len(train_dataloader))\n",
    "        for _ in progress:\n",
    "            engine.zero_grad()\n",
    "            engine.execute_schedule(data_iter, return_output_label=False)\n",
    "            engine.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Trial with LARS Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAKgExC5CI4x",
    "outputId": "6a00635d-1c07-44f3-f3aa-bcf310bb44b8",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example trial (20 steps) with lars optimizer\n",
    "!colossalai run --nproc_per_node 2 train.py --config config.py --optimizer lars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Trial with LAMB Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5fK72FRCigM",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now use Lamb optimizer\n",
    "!colossalai run --nproc_per_node 2 train.py --config config.py --optimizer lamb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "933351abc630374ba7d69d29305d8827306c5eb75384e02fe79e055db4d99722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
